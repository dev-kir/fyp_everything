[0;32m========================================[0m
[0;32mSwarmGuard Scenario 2 - Ultimate Test[0m
[0;32m========================================[0m

[0;34mConfiguration:[0m
  Alpine nodes:       5
  Users per Alpine:   12
  Total users:        60
  Stagger delay:      2s (between user starts)
  Ramp time:          60s (per user, 0â†’max)
  Hold time:          900s (maintain peak load)

[0;34mPer-User Resource Contribution:[0m
  CPU:     2%
  Memory:  8MB
  Network: 12Mbps

[0;34mExpected Peak Load (All Users Active):[0m
  Total CPU:     120% \033[1;33m(Scenario 2 threshold: 75%)\033[0m
  Total Memory:  480MB \033[1;33m(Scenario 2 threshold: 80% node memory)\033[0m
  Total Network: 720Mbps \033[1;33m(Scenario 2 threshold: 65Mbps)\033[0m

[0;34mTimeline:[0m
  T+0s:      User 1 starts on each Alpine (4 users total)
  T+2s:     User 2 starts on each Alpine (8 users total)
  T+82s:  All 60 users active, ramping complete
  T+??s:     Scenario 2 triggers â†’ Scale 1 â†’ 2+ replicas
  T+982s:  Test completes, resources release
  T+1162s: Scale-down cooldown â†’ Back to 1 replica

[1;33m[0/5] Pre-cleanup: Killing any leftover processes...[0m
[0;32mâœ“ Pre-cleanup complete[0m

[1;33m[1/5] Checking initial state...[0m
  Current web-stress replicas: 1

[1;33m[2/5] Checking service health...[0m
[0;32mâœ“ Service healthy[0m

[1;33m[3/5] Creating Alpine simulation script...[0m
[0;32mâœ“ Script created[0m

[1;33m[4/5] Deploying to Alpine nodes...[0m
  Deploying to alpine-1...
  Deploying to alpine-2...
  Deploying to alpine-3...
  Deploying to alpine-4...
  Deploying to alpine-5...
[0;32mâœ“ Deployed to 5 Alpine nodes[0m

[0;32m========================================[0m
[0;32m[5/5] Starting Scenario 2 Test[0m
[0;32m========================================[0m

[1;33mğŸ“Š OPEN GRAFANA NOW:[0m
   [0;34mhttp://192.168.2.61:3000[0m
   Dashboard: [0;34mSwarmGuard_All_Sum[0m

[1;33mExpected Behavior:[0m
  Phase 1: Gradual resource ramp-up (82s)
    - CPU, Memory, Network all increase smoothly
    - Each user adds load in staggered fashion

  Phase 2: Scenario 2 triggers (around T+60-90s)
    - Recovery manager detects: CPU > 75% AND Network > 65Mbps
    - Scales web-stress: 1 â†’ 2 replicas

  Phase 3: Load distribution visible in Grafana
    - Before: 1 replica at ~120% CPU, ~480MB RAM
    - After:  2 replicas at ~60% CPU each, ~240MB RAM each
    - LB Dashboard shows requests distributed across both replicas

  Phase 4: Hold peak load (900s)
    - Maintain distributed load to show stability

  Phase 5: Cool down and scale-down
    - After test completes, resources release
    - Recovery manager waits 180s cooldown
    - Scales back: 2 â†’ 1 replica

[1;33mPress Ctrl+C to stop early[0m

[0;34m  Starting 12 users on alpine-1...[0m
[0;34m  Starting 12 users on alpine-2...[0m
[0;34m  Starting 12 users on alpine-3...[0m
[0;34m  Starting 12 users on alpine-4...[0m
[0;34m  Starting 12 users on alpine-5...[0m

[0;32mâœ“ 60 users triggered across 5 Alpine nodes[0m

[1;33mMonitoring for 982s...[0m

[T+10s] Ramping... | Active users: ~30/60 | Replicas: 1
[T+20s] Ramping... | Active users: ~55/60 | Replicas: 1
[T+30s] Ramping... | Active users: ~60/60 | Replicas: 1
[T+41s] Ramping... | Active users: ~60/60 | Replicas: 1
[T+51s] Ramping... | Active users: ~60/60 | Replicas: 1
[T+61s] Ramping... | Active users: ~60/60 | Replicas: 1
[T+71s] Ramping... | Active users: ~60/60 | Replicas: 1
[T+82s] Peak load | Replicas: 1 | Monitoring for scale events...
[T+92s] Peak load | Replicas: 1 | Monitoring for scale events...
[T+102s] Peak load | Replicas: 1 | Monitoring for scale events...
[T+112s] Peak load | Replicas: 1 | Monitoring for scale events...
[T+122s] Peak load | Replicas: 1 | Monitoring for scale events...
[T+132s] Peak load | Replicas: 1 | Monitoring for scale events...
[T+143s] Peak load | Replicas: 1 | Monitoring for scale events...
[T+153s] Peak load | Replicas: 1 | Monitoring for scale events...
[T+163s] Peak load | Replicas: 1 | Monitoring for scale events...
[T+173s] Peak load | Replicas: 1 | Monitoring for scale events...
[T+183s] Peak load | Replicas: 1 | Monitoring for scale events...
[T+194s] Peak load | Replicas: 1 | Monitoring for scale events...
[T+204s] Peak load | Replicas: 1 | Monitoring for scale events...
[T+214s] Peak load | Replicas: 1 | Monitoring for scale events...

[0;32mâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—[0m
[0;32mâ•‘  âœ… SCALE EVENT DETECTED!                              â•‘[0m
[0;32mâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•[0m
  Time:      T+224s
  Change:    1 â†’ 2 replicas

[1;33m  Expected load distribution:[0m
    - Each replica now handles ~60% CPU
    - Each replica now handles ~240MB Memory
    - Each replica now handles ~360Mbps Network

[1;33m  Check Grafana to verify distribution![0m


[0;32mâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—[0m
[0;32mâ•‘  âœ… SCALE EVENT DETECTED!                              â•‘[0m
[0;32mâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•[0m
  Time:      T+234s
  Change:    2 â†’ 1 replicas

[1;33m  Expected load distribution:[0m
    - Each replica now handles ~120% CPU
    - Each replica now handles ~480MB Memory
    - Each replica now handles ~720Mbps Network

[1;33m  Check Grafana to verify distribution![0m


[0;32mâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—[0m
[0;32mâ•‘  âœ… SCALE EVENT DETECTED!                              â•‘[0m
[0;32mâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•[0m
  Time:      T+245s
  Change:    1 â†’ 2 replicas

[1;33m  Expected load distribution:[0m
    - Each replica now handles ~60% CPU
    - Each replica now handles ~240MB Memory
    - Each replica now handles ~360Mbps Network

[1;33m  Check Grafana to verify distribution![0m

[T+255s] Peak load | Replicas: 2 | Monitoring for scale events...
[T+265s] Peak load | Replicas: 2 | Monitoring for scale events...
[T+275s] Peak load | Replicas: 2 | Monitoring for scale events...
[T+285s] Peak load | Replicas: 2 | Monitoring for scale events...
[T+296s] Peak load | Replicas: 2 | Monitoring for scale events...
[T+306s] Peak load | Replicas: 2 | Monitoring for scale events...
[T+316s] Peak load | Replicas: 2 | Monitoring for scale events...
[T+326s] Peak load | Replicas: 2 | Monitoring for scale events...
[T+336s] Peak load | Replicas: 2 | Monitoring for scale events...
[T+347s] Peak load | Replicas: 2 | Monitoring for scale events...
[T+357s] Peak load | Replicas: 2 | Monitoring for scale events...
[T+367s] Peak load | Replicas: 2 | Monitoring for scale events...
[T+377s] Peak load | Replicas: 2 | Monitoring for scale events...
[T+387s] Peak load | Replicas: 2 | Monitoring for scale events...
[T+398s] Peak load | Replicas: 2 | Monitoring for scale events...
[T+408s] Peak load | Replicas: 2 | Monitoring for scale events...
[T+418s] Peak load | Replicas: 2 | Monitoring for scale events...
[T+428s] Peak load | Replicas: 2 | Monitoring for scale events...
[T+438s] Peak load | Replicas: 2 | Monitoring for scale events...
[T+449s] Peak load | Replicas: 2 | Monitoring for scale events...
[T+459s] Peak load | Replicas: 2 | Monitoring for scale events...
[T+469s] Peak load | Replicas: 2 | Monitoring for scale events...
[T+479s] Peak load | Replicas: 2 | Monitoring for scale events...
[T+489s] Peak load | Replicas: 2 | Monitoring for scale events...
[T+500s] Peak load | Replicas: 2 | Monitoring for scale events...
[T+510s] Peak load | Replicas: 2 | Monitoring for scale events...

[0;32mâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—[0m
[0;32mâ•‘  âœ… SCALE EVENT DETECTED!                              â•‘[0m
[0;32mâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•[0m
  Time:      T+520s
  Change:    2 â†’ 3 replicas

[1;33m  Expected load distribution:[0m
    - Each replica now handles ~40% CPU
    - Each replica now handles ~160MB Memory
    - Each replica now handles ~240Mbps Network

[1;33m  Check Grafana to verify distribution![0m

[T+530s] Peak load | Replicas: 3 | Monitoring for scale events...
[T+540s] Peak load | Replicas: 3 | Monitoring for scale events...
[T+550s] Peak load | Replicas: 3 | Monitoring for scale events...
[T+561s] Peak load | Replicas: 3 | Monitoring for scale events...
[T+571s] Peak load | Replicas: 3 | Monitoring for scale events...
[T+581s] Peak load | Replicas: 3 | Monitoring for scale events...
[T+591s] Peak load | Replicas: 3 | Monitoring for scale events...
[T+601s] Peak load | Replicas: 3 | Monitoring for scale events...
[T+612s] Peak load | Replicas: 3 | Monitoring for scale events...
[T+622s] Peak load | Replicas: 3 | Monitoring for scale events...
[T+632s] Peak load | Replicas: 3 | Monitoring for scale events...
[T+642s] Peak load | Replicas: 3 | Monitoring for scale events...
[T+652s] Peak load | Replicas: 3 | Monitoring for scale events...
[T+663s] Peak load | Replicas: 3 | Monitoring for scale events...
[T+673s] Peak load | Replicas: 3 | Monitoring for scale events...
[T+683s] Peak load | Replicas: 3 | Monitoring for scale events...
[T+693s] Peak load | Replicas: 3 | Monitoring for scale events...

[1;33m[Cleanup] Stopping all Alpine traffic...[0m
[1;33m[Cleanup] Stopping stress on containers...[0m
[0;32mâœ“ Cleanup complete[0m
[T+722s] Peak load | Replicas: 3 | Monitoring for scale events...
[T+733s] Peak load | Replicas: 3 | Monitoring for scale events...
[T+743s] Peak load | Replicas: 3 | Monitoring for scale events...

[0;32mâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—[0m
[0;32mâ•‘  âœ… SCALE EVENT DETECTED!                              â•‘[0m
[0;32mâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•[0m
  Time:      T+753s
  Change:    3 â†’ 2 replicas

[1;33m  Expected load distribution:[0m
    - Each replica now handles ~60% CPU
    - Each replica now handles ~240MB Memory
    - Each replica now handles ~360Mbps Network

[1;33m  Check Grafana to verify distribution![0m

[T+763s] Peak load | Replicas: 2 | Monitoring for scale events...
[T+774s] Peak load | Replicas: 2 | Monitoring for scale events...
[T+784s] Peak load | Replicas: 2 | Monitoring for scale events...
[T+794s] Peak load | Replicas: 2 | Monitoring for scale events...
[T+804s] Peak load | Replicas: 2 | Monitoring for scale events...
[T+814s] Peak load | Replicas: 2 | Monitoring for scale events...
[T+824s] Peak load | Replicas: 2 | Monitoring for scale events...
[T+835s] Peak load | Replicas: 2 | Monitoring for scale events...
[T+845s] Peak load | Replicas: 2 | Monitoring for scale events...
[T+855s] Peak load | Replicas: 2 | Monitoring for scale events...
[T+865s] Peak load | Replicas: 2 | Monitoring for scale events...
[T+875s] Peak load | Replicas: 2 | Monitoring for scale events...
[T+886s] Peak load | Replicas: 2 | Monitoring for scale events...
[T+896s] Peak load | Replicas: 2 | Monitoring for scale events...
[T+906s] Peak load | Replicas: 2 | Monitoring for scale events...
[T+916s] Peak load | Replicas: 2 | Monitoring for scale events...
[T+926s] Peak load | Replicas: 2 | Monitoring for scale events...
[T+937s] Peak load | Replicas: 2 | Monitoring for scale events...
[T+947s] Peak load | Replicas: 2 | Monitoring for scale events...
[T+957s] Peak load | Replicas: 2 | Monitoring for scale events...
[T+967s] Peak load | Replicas: 2 | Monitoring for scale events...

[0;32mâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—[0m
[0;32mâ•‘  âœ… SCALE EVENT DETECTED!                              â•‘[0m
[0;32mâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•[0m
  Time:      T+977s
  Change:    2 â†’  replicas

[1;33m  Expected load distribution:[0m
./tests/scenario2_ultimate.sh: line 361: TOTAL_CPU / CURRENT_REPLICAS: division by 0 (error token is "S")

[1;33mWaiting for Alpine nodes to complete...[0m

[0;32m========================================[0m
[0;32mTest Complete![0m
[0;32m========================================[0m

[0;34mSummary:[0m
  Initial replicas:  1
  Final replicas:    
  Total users:       60
  Expected peak:     120% CPU, 480MB RAM, 720Mbps NET
  Scale-up time:     T+753s

[0;34mCurrent replica distribution:[0m
no such service: web-stress

[1;33m[Cleanup] Stopping all Alpine traffic...[0m
[1;33m[Cleanup] Stopping stress on containers...[0m
[0;32mâœ“ Cleanup complete[0m
