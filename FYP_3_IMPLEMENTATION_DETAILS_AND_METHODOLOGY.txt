================================================================================
SWARMGUARD: IMPLEMENTATION DETAILS AND DEVELOPMENT METHODOLOGY
================================================================================

PART 3: IMPLEMENTATION, DEVELOPMENT PROCESS, AND METHODOLOGY
(Foundation for Chapter 3: Methodology - Implementation Approach)

================================================================================
1. DEVELOPMENT METHODOLOGY AND ITERATIVE APPROACH
================================================================================

1.1 Development Philosophy

The SwarmGuard project followed an iterative, problem-solving approach rather
than a traditional waterfall methodology. This approach was necessary because:

1. Docker Swarm API Limitations:
   - Official documentation incomplete for advanced operations
   - Python Docker SDK behavior undocumented for many parameters
   - Required extensive experimentation to discover working solutions

2. Zero-Downtime Challenge:
   - No existing examples of proactive migration in Docker Swarm
   - Multiple failed approaches before achieving zero downtime
   - Each failure provided insights leading to eventual solution

3. Real-World Testing Requirements:
   - Theory had to be validated on physical hardware
   - Network constraints (100Mbps) required actual measurement
   - Performance targets required iterative optimization

METHODOLOGY SUMMARY:
- Problem-driven: Each iteration solved a specific blocking issue
- Evidence-based: Decisions based on actual test results, not assumptions
- Incremental: Built working solution piece by piece
- Well-documented: IMPLEMENTATION_LOG.md records all 28 attempts with rationale

1.2 Major Implementation Phases

PHASE 1: Foundation (Attempts 1-5)
Goal: Establish basic container migration capability
Outcome: Learned that high-level Docker SDK APIs insufficient

Key Attempts:
- Attempt 1: service.update() with placement constraints → Failed (parameter error)
- Attempt 2: Docker CLI via subprocess → Failed (too slow, installation issues)
- Attempt 3: Scale up/down → Failed (Docker removes wrong task)
- Attempt 4: Direct task deletion via requests-unixsocket → Failed (URL encoding)
- Attempt 5: Docker SDK low-level API → Partial success

Learning: Container migration in Docker Swarm requires rolling updates, not
          scale operations

PHASE 2: Rolling Update Approach (Attempts 6-10)
Goal: Use Docker Swarm's rolling update mechanism for migration
Outcome: Achieved migration but with downtime

Key Attempts:
- Attempt 6: Increased wait timeout (15s → 30s) → Fixed timeout issues
- Attempt 7: service.update() with force_update → Migrations working!
- Attempt 8: Fix constraint accumulation → Prevents node exclusion
- Attempt 9: Stale alert detection → Prevents duplicate migrations
- Attempt 10: Placement constraints before scaling → Same-node placement issue

Learning: Constraint updates trigger rolling updates, causing downtime

PHASE 3: Zero-Downtime Achievement (Attempts 11-17)
Goal: Eliminate downtime during migration
Outcome: Achieved true zero downtime through START-FIRST ordering

Key Attempts:
- Attempt 11: Constraint update causing premature shutdown → 6s downtime
- Attempt 12: No constraints, rely on spread strategy → Unreliable
- Attempt 13: Constraints + grace period → Race condition, 18s downtime
- Attempt 14: 5-second wait between operations → Still race condition
- Attempt 15: Docker native rolling update → STOP-FIRST caused 19s downtime
- Attempt 16: START-FIRST order via low-level API → force_update error
- Attempt 17: ForceUpdate counter in TaskTemplate → SUCCESS!

Learning: Low-level Docker API required to configure START-FIRST update order

PHASE 4: Scenario 2 Implementation (Attempts 18-20)
Goal: Implement horizontal scaling with automatic scale-down
Outcome: Complete autoscaling loop with background monitoring

Key Attempts:
- Attempt 18: Network percentage fix + background scale-down thread
- Attempt 19: OR logic for CPU/Memory (easier testing, more realistic)
- Attempt 20: Fix scale-down threshold boundary issue

Learning: Background thread necessary for scale-down detection (no event trigger)

PHASE 5: Full System Integration (Attempts 21-28)
Goal: Distributed load testing and visualization
Outcome: Working system with comprehensive testing infrastructure

Key Attempts:
- Attempt 21: Scenario 2 full integration testing → Both scale-up and scale-down working
- Attempt 22: Constraint cleanup after migration → Better replica distribution
- Attempt 23: Alpine Pi distributed load testing scripts
- Attempt 24: Hybrid stress approach (self-stress + external traffic)
- Attempt 25: Configurable test scripts with parameters
- Attempt 26: Multiple stress requests to distribute load across replicas
- Attempt 27: External traffic for CPU distribution (not self-stress)
- Attempt 28: Continuous traffic model for real-time distribution observation

Learning: External traffic (Alpine Pi requests) distributed by Docker load
          balancer, internal stress endpoints are not

1.3 Research Method: Empirical Problem-Solving

The development process exemplifies empirical research methodology:

HYPOTHESIS → EXPERIMENT → OBSERVATION → REFINEMENT

Example: Achieving Zero Downtime (Attempts 10-17)

Hypothesis 1: "Placement constraints will force new task to different node"
Experiment: Add constraint, then scale 1→2
Observation: New task placed on same node anyway
Conclusion: Constraint alone insufficient, Docker spread strategy unreliable
Refinement: Need to prevent same-node placement explicitly

Hypothesis 2: "Adding constraint and scaling together will work"
Experiment: service.update(constraints=...) + service.scale(2) immediately
Observation: "update out of sequence" error, 18s downtime
Conclusion: Race condition between update and scale operations
Refinement: Need single atomic operation

Hypothesis 3: "5-second wait between operations will prevent race condition"
Experiment: update → sleep(5) → scale
Observation: Still "update out of sequence", still 18s downtime
Conclusion: Wait time doesn't solve underlying problem
Refinement: Must use different approach entirely

Hypothesis 4: "Docker's native rolling update can handle this"
Experiment: service.update(force_update=True)
Observation: 19s downtime, old task stopped before new started
Conclusion: Default update_order is STOP-FIRST
Refinement: Must configure START-FIRST order

Hypothesis 5: "Low-level API with START-FIRST order will achieve zero downtime"
Experiment: api.update_service() with UpdateConfig Order: 'start-first'
Observation: SUCCESS - both tasks running during transition, 0s downtime
Conclusion: Low-level API required for advanced update configurations

This iterative refinement process demonstrates scientific method applied to
systems engineering.

================================================================================
2. IMPLEMENTATION DETAILS BY COMPONENT
================================================================================

2.1 Monitoring Agent Implementation

FILE STRUCTURE:
- agent.py: Main event loop and orchestration
- metrics_collector.py: Docker stats API interaction
- influxdb_writer.py: Batched writes to InfluxDB
- alert_sender.py: HTTP alerts to recovery manager
- Dockerfile: Container image definition
- requirements.txt: Python dependencies

CORE ALGORITHM (agent.py):

```
INITIALIZATION:
  Load configuration from environment variables
  Initialize Docker client (connect to /var/run/docker.sock)
  Initialize InfluxDB writer (HTTP client)
  Initialize Alert sender (HTTP client with keepalive)
  Set poll_interval = 5 seconds
  Register signal handlers for graceful shutdown

MAIN LOOP (runs every 5 seconds):
  WHILE running:
    START iteration_timer

    # Step 1: Collect metrics from all containers
    metrics = metrics_collector.collect()
    # Returns: {
    #   'node': {cpu, memory, ...},
    #   'containers': [
    #     {container_id, name, service, cpu_percent, memory_mb, memory_percent,
    #      network_rx_mbps, network_tx_mbps},
    #     ...
    #   ]
    # }

    # Step 2: Process each container
    FOR EACH container IN metrics['containers']:
      # Calculate network percentage
      net_total_mbps = container.network_rx_mbps + container.network_tx_mbps
      net_percent = (net_total_mbps / 100.0) * 100  # 100Mbps capacity

      # Step 3: Check thresholds
      scenario = None
      IF (cpu > 75 OR memory > 80) AND net_percent < 35:
        scenario = "scenario1_migration"
        LOG "Scenario 1 detected"

      ELIF (cpu > 75 OR memory > 80) AND net_percent > 65:
        scenario = "scenario2_scaling"
        LOG "Scenario 2 detected"

      # Step 4: Send alert if threshold violated
      IF scenario IS NOT None:
        alert_data = {
          timestamp, node, container_id, service_name, scenario,
          metrics: {cpu_percent, memory_mb, memory_percent,
                    network_rx_mbps, network_tx_mbps, network_percent}
        }
        ASYNC alert_sender.send_alert(alert_data)  # Non-blocking

      # Step 5: Batch metrics for InfluxDB
      metrics_batch.append(container_metrics)

    # Step 6: Flush batch to InfluxDB if needed
    IF time_since_last_flush > 10 seconds OR batch_size > 20:
      ASYNC influxdb_writer.write_batch(metrics_batch)  # Non-blocking
      metrics_batch.clear()
      last_flush = now()

    # Step 7: Sleep until next interval
    elapsed = iteration_timer.elapsed()
    IF elapsed < poll_interval:
      SLEEP(poll_interval - elapsed)
    ELSE:
      LOG WARNING "Collection took longer than poll_interval"

  # Cleanup on shutdown
  influxdb_writer.flush_remaining()
  LOG "Monitoring agent stopped"
```

METRICS COLLECTION DETAILS (metrics_collector.py):

```
FUNCTION collect_metrics():
  metrics = {node: {}, containers: []}

  # Node-level metrics
  metrics.node.hostname = NODE_NAME
  metrics.node.timestamp = current_timestamp()

  # Query all containers on this node
  containers = docker_client.containers.list()

  FOR EACH container IN containers:
    # Skip monitoring agents and recovery manager
    IF container.name CONTAINS "monitoring-agent" OR "recovery-manager":
      CONTINUE

    # Get real-time stats (non-streaming)
    stats = docker_client.api.stats(container.id, stream=False)

    # Parse CPU percentage
    cpu_delta = stats.cpu_stats.cpu_usage.total_usage -
                stats.precpu_stats.cpu_usage.total_usage
    system_delta = stats.cpu_stats.system_cpu_usage -
                   stats.precpu_stats.system_cpu_usage
    cpu_percent = (cpu_delta / system_delta) * num_cpus * 100
    cpu_percent = min(cpu_percent, 100.0)  # Normalize to 0-100%

    # Parse memory
    memory_usage_mb = stats.memory_stats.usage / (1024 * 1024)
    memory_limit_mb = stats.memory_stats.limit / (1024 * 1024)
    memory_percent = (memory_usage_mb / memory_limit_mb) * 100

    # Parse network stats from /sys/class/net/{NET_IFACE}/statistics/
    rx_bytes = read_file(f"/sys/class/net/{NET_IFACE}/statistics/rx_bytes")
    tx_bytes = read_file(f"/sys/class/net/{NET_IFACE}/statistics/tx_bytes")

    # Calculate bandwidth (compare with previous measurement)
    time_delta = current_time - previous_time
    rx_mbps = ((rx_bytes - previous_rx) * 8 / time_delta) / 1_000_000
    tx_mbps = ((tx_bytes - previous_tx) * 8 / time_delta) / 1_000_000

    # Extract service name from container labels
    service_name = container.labels.get("com.docker.swarm.service.name", "")

    metrics.containers.append({
      container_id: container.id[:12],
      container_name: container.name,
      service_name: service_name,
      cpu_percent: cpu_percent,
      memory_mb: memory_usage_mb,
      memory_percent: memory_percent,
      network_rx_mbps: rx_mbps,
      network_tx_mbps: tx_mbps
    })

  RETURN metrics
```

KEY IMPLEMENTATION CHALLENGES AND SOLUTIONS:

Challenge 1: CPU Percentage Normalization
Problem: Raw Docker stats return CPU usage that can exceed 100% on multi-core
         systems (e.g., 800% on 8-core machine)
Solution: Normalize to 0-100% by dividing by number of CPU cores and capping
          at 100%
Code: cpu_percent = min((cpu_delta / system_delta) * num_cpus * 100, 100.0)
Impact: Consistent threshold comparison across nodes with different core counts

Challenge 2: Network Interface Discovery
Problem: Each node has different network interface name (enp5s0f0, eno1, etc.)
Solution: Pass interface name as environment variable (NET_IFACE)
Configuration: Deployment scripts set NET_IFACE per node
Impact: Works across heterogeneous cluster

Challenge 3: Async HTTP Without Blocking
Problem: Synchronous InfluxDB writes and alert sends block metrics collection
Solution: Use aiohttp for async HTTP, run event loop in main thread
Code: await alert_sender.send_alert() / await influxdb_writer.write_batch()
Impact: Collection continues even if HTTP requests pending

2.2 Recovery Manager Implementation

FILE STRUCTURE:
- manager.py: Flask HTTP server and alert handler
- docker_controller.py: Docker Swarm API operations
- rule_engine.py: Threshold-based decision logic
- config_loader.py: YAML configuration parser
- Dockerfile: Container image definition
- config.yaml: Default configuration

CORE ALGORITHM (manager.py):

```
INITIALIZATION:
  Load configuration from /app/config.yaml
  Initialize Docker client (connect to /var/run/docker.sock)
  Initialize rule engine with threshold configuration
  Initialize state tracking:
    - cooldowns = {}  # {service_name: last_action_timestamp}
    - breach_counts = {}  # {container_id: consecutive_breaches}
    - scale_down_last_checked = {}  # {service_name: {idle_since, checked_at}}
  Start background scale-down monitoring thread
  Start Flask HTTP server on port 5000

HTTP ENDPOINT: POST /alert
  REQUEST BODY: {
    timestamp, node, container_id, service_name, scenario,
    metrics: {cpu_percent, memory_percent, network_percent, ...}
  }

  RESPONSE: {status, message, breach_count, action_taken, ...}

  LOGIC:
    START timer

    # Step 1: Consecutive breach tracking (reduce false positives)
    IF container_id NOT IN breach_counts:
      breach_counts[container_id] = 0
    breach_counts[container_id] += 1

    IF breach_counts[container_id] < 2:  # Require 2 consecutive breaches
      LOG "Breach {count}/2 for {service} - waiting"
      RETURN {status: "waiting", breach_count: count}

    breach_counts[container_id] = 0  # Reset counter

    # Step 2: Cooldown check (prevent flapping)
    current_time = time.time()
    IF service_name IN cooldowns:
      IF scenario == "scenario1_migration":
        cooldown_period = 60  # 60 seconds for migration
      ELIF scenario == "scenario2_scaling":
        cooldown_period = 60  # 60 seconds for scale-up
      ELIF scenario == "scenario2_scale_down":
        cooldown_period = 180  # 180 seconds for scale-down
      ELSE:
        cooldown_period = 30  # Default

      time_since_last = current_time - cooldowns[service_name]
      IF time_since_last < cooldown_period:
        LOG "Cooldown active: {time_since_last}s < {cooldown_period}s"
        RETURN {status: "cooldown"}

    # Step 3: Scenario-specific recovery action
    ACQUIRE lock  # Prevent concurrent modifications

    IF scenario == "scenario1_migration":
      result = execute_migration(service_name, container_id, node)

    ELIF scenario == "scenario2_scale_up" OR "scenario2_scaling":
      result = execute_scale_up(service_name)

    ELIF scenario == "scenario2_scale_down":
      result = execute_scale_down(service_name)

    ELSE:
      result = {status: "error", message: "Unknown scenario"}

    RELEASE lock

    # Step 4: Update cooldown timestamp
    cooldowns[service_name] = current_time

    # Step 5: Log timing
    elapsed_ms = (time.time() - timer) * 1000
    LOG "Alert processed in {elapsed_ms:.0f}ms"
    IF elapsed_ms > 1000:
      LOG WARNING "Processing exceeded 1 second target"

    RETURN result
```

MIGRATION ALGORITHM (docker_controller.py):

This is the MOST CRITICAL function - achieving zero-downtime migration.

```
FUNCTION migrate_container(service_name, from_node):
  LOG "Executing migration for {service_name} from {from_node}"
  migration_start = time.time()

  # Step 1: Verify container still on reported node (stale alert detection)
  actual_node = get_service_node(service_name)
  IF actual_node != from_node:
    LOG "Stale alert ignored: reported {from_node}, actually on {actual_node}"
    RETURN {status: "stale_alert"}

  # Step 2: Get service details
  service = docker_client.services.get(service_name)
  service_spec = service.attrs['Spec']
  task_template = service_spec['TaskTemplate']
  current_image = task_template['ContainerSpec']['Image']

  # Step 3: Configure placement constraints
  placement = task_template.get('Placement', {})
  current_constraints = placement.get('Constraints', [])

  # Remove any previous migration constraints
  base_constraints = [c for c in current_constraints
                       if 'node.hostname!=' not in c]

  # Add constraint to avoid problem node
  new_constraints = base_constraints + [f'node.hostname!={from_node}']

  task_template['Placement'] = {'Constraints': new_constraints}

  # Step 4: Increment ForceUpdate counter (force task recreation)
  IF 'ForceUpdate' NOT IN task_template:
    task_template['ForceUpdate'] = 0
  task_template['ForceUpdate'] += 1

  # Step 5: Configure START-FIRST rolling update
  update_config = {
    'Parallelism': 1,       # One task at a time
    'Delay': 0,             # No delay between updates
    'Order': 'start-first', # KEY: Start new before stopping old
    'FailureAction': 'pause'
  }

  # Step 6: Trigger rolling update via low-level API
  version = service.version
  docker_client.api.update_service(
    service.id,
    version=version,
    task_template=task_template,
    update_config=update_config,
    mode=service_spec.get('Mode'),
    networks=service_spec.get('Networks'),
    endpoint_spec=service_spec.get('EndpointSpec')
  )

  LOG "Rolling update initiated with START-FIRST ordering"

  # Step 7: Monitor update progress (wait for new task)
  max_wait = 40  # Maximum 40 seconds
  start_time = time.time()

  WHILE (time.time() - start_time) < max_wait:
    tasks = service.tasks(filters={'desired-state': 'running'})
    running_tasks = [t for t in tasks if t['Status']['State'] == 'running']

    # Check if migration complete
    IF len(running_tasks) == 1:
      new_task_node = running_tasks[0]['NodeID']
      new_node_name = get_node_name(new_task_node)

      IF new_node_name != from_node:
        # Migration successful!
        migration_duration = time.time() - migration_start
        LOG "✅ Migration successful: {from_node} → {new_node_name}"
        LOG "MTTR: {migration_duration:.2f}s"

        # Step 8: Clean up migration constraints (restore normal scheduling)
        current_spec = service.attrs['Spec']
        current_template = current_spec['TaskTemplate']
        current_placement = current_template.get('Placement', {})
        current_constraints_after = current_placement.get('Constraints', [])

        cleaned_constraints = [c for c in current_constraints_after
                                if 'node.hostname!=' not in c]

        IF len(cleaned_constraints) != len(current_constraints_after):
          current_template['Placement']['Constraints'] = cleaned_constraints
          service.update(task_template=current_template, version=service.version)
          LOG "✅ Migration constraints removed - normal scheduling restored"

        RETURN {
          status: "success",
          from_node: from_node,
          to_node: new_node_name,
          mttr_seconds: migration_duration
        }

    SLEEP 2  # Check every 2 seconds

  # Timeout - migration took too long
  LOG ERROR "Migration timeout after {max_wait}s"
  RETURN {status: "timeout", message: "Migration exceeded maximum wait time"}
```

SCALE-UP ALGORITHM (docker_controller.py):

```
FUNCTION scale_up(service_name):
  LOG "Executing scale-up for {service_name}"
  scale_start = time.time()

  # Step 1: Get current replica count
  service = docker_client.services.get(service_name)
  mode = service.attrs['Spec'].get('Mode', {})
  current_replicas = mode.get('Replicated', {}).get('Replicas', 1)
  new_replicas = current_replicas + 1

  # Step 2: Check maximum replicas limit
  max_replicas = config.get('scenarios.scenario2_scaling.max_replicas', 10)
  IF new_replicas > max_replicas:
    LOG "Max replicas reached: {current_replicas}/{max_replicas}"
    RETURN {status: "max_replicas", current: current_replicas}

  # Step 3: Scale service
  LOG "Scaling {service_name}: {current_replicas} → {new_replicas}"
  service.scale(new_replicas)

  # Step 4: Wait for new replica to be running
  max_wait = 30
  start_time = time.time()

  WHILE (time.time() - start_time) < max_wait:
    service.reload()
    tasks = service.tasks(filters={'desired-state': 'running'})
    running_tasks = [t for t in tasks if t['Status']['State'] == 'running']

    IF len(running_tasks) >= new_replicas:
      scale_duration = time.time() - scale_start
      LOG "✅ Scale-up successful: {current_replicas} → {new_replicas}"
      LOG "Duration: {scale_duration:.2f}s"
      RETURN {
        status: "success",
        from_replicas: current_replicas,
        to_replicas: new_replicas,
        duration_seconds: scale_duration
      }

    SLEEP 1

  # Timeout
  LOG ERROR "Scale-up timeout after {max_wait}s"
  RETURN {status: "timeout"}
```

BACKGROUND SCALE-DOWN THREAD (manager.py):

```
FUNCTION monitor_scale_down_thread():
  LOG "Background scale-down monitoring started"

  WHILE running:
    TRY:
      SLEEP 60  # Check every 60 seconds

      # Get all services eligible for autoscaling (replicas > 1)
      autoscaling_services = docker_controller.get_autoscaling_services()

      FOR EACH service_name IN autoscaling_services:
        # Get aggregate metrics across all replicas
        metrics = docker_controller.get_service_aggregate_metrics(service_name)
        # Returns: {replicas, total_cpu, total_mem, avg_cpu, avg_mem}

        # Apply PRD scale-down formula:
        # Can scale down if: total_usage < threshold × (N - 1)
        cpu_threshold = config.get('scenarios.scenario2_scaling.cpu_threshold', 75)
        mem_threshold = config.get('scenarios.scenario2_scaling.memory_threshold', 80)

        can_scale_down = (
          metrics.total_cpu < cpu_threshold * (metrics.replicas - 1) AND
          metrics.total_mem < mem_threshold * (metrics.replicas - 1)
        )

        IF can_scale_down:
          # Track idle duration
          IF service_name NOT IN scale_down_last_checked:
            scale_down_last_checked[service_name] = {
              idle_since: time.time(),
              checked_at: time.time()
            }
            LOG "Service {service_name} idle detected, will scale down after 180s"

          ELSE:
            idle_duration = time.time() - scale_down_last_checked[service_name].idle_since
            IF idle_duration >= 180:  # 180 seconds sustained idle
              # Check cooldown
              IF service_name IN cooldowns:
                time_since_last = time.time() - cooldowns[service_name]
                IF time_since_last < 180:
                  LOG "Scale-down cooldown active"
                  CONTINUE

              # Execute scale-down
              LOG "Scale-down triggered after {idle_duration}s idle"
              result = execute_scale_down(service_name)

              IF result.status == "success":
                cooldowns[service_name] = time.time()
                DELETE scale_down_last_checked[service_name]

        ELSE:
          # Not idle - reset tracking
          IF service_name IN scale_down_last_checked:
            DELETE scale_down_last_checked[service_name]

    CATCH Exception as e:
      LOG ERROR "Scale-down monitoring error: {e}"

  LOG "Background scale-down monitoring stopped"
```

KEY IMPLEMENTATION CHALLENGES AND SOLUTIONS:

Challenge 1: START-FIRST Not Available in High-Level API
Problem: service.update() doesn't accept update_order parameter
Solution: Use low-level api.update_service() with UpdateConfig dict
Impact: Required understanding Docker API structure at protocol level

Challenge 2: force_update Parameter Error in Low-Level API
Problem: api.update_service() doesn't accept force_update=True kwarg
Error: "got an unexpected keyword argument 'force_update'"
Solution: Increment ForceUpdate field in TaskTemplate spec
Impact: Learned difference between high-level and low-level API parameters

Challenge 3: Constraint Accumulation
Problem: Each migration added new constraint without removing old ones
Impact: Eventually all nodes excluded, nowhere to migrate
Solution: Filter out previous 'node.hostname!=' constraints before adding new one
Code: base = [c for c in constraints if 'node.hostname!=' not in c]

Challenge 4: Stale Alert Double Migration
Problem: Alert sent at T+0 (on worker-3), migration completes at T+20 (now on
         worker-4), then stale alert processed triggers another migration
Solution: Before executing migration, verify container still on reported node
Code: actual_node = get_service_node(); if actual != reported: return
Impact: Prevents redundant recovery actions, saves resources

2.3 Web-Stress Test Application Implementation

FILE STRUCTURE:
- app.py: FastAPI application and endpoint definitions
- metrics.py: Real-time system metrics (CPU, memory, network)
- stress/cpu_stress.py: CPU stress implementation
- stress/memory_stress.py: Memory allocation and retention
- stress/network_stress.py: Network traffic generation
- Dockerfile: Container image
- requirements.txt: Dependencies

CORE IMPLEMENTATION (app.py):

```
INITIALIZATION:
  app = FastAPI()
  active_stresses = {}  # Track running stress tests

ENDPOINT: GET /stress/cpu
  PARAMETERS: target (0-100), duration (seconds), ramp (seconds)

  LOGIC:
    # Spawn CPU stress in background thread
    thread = Thread(target=cpu_stress_worker, args=(target, duration, ramp))
    thread.daemon = True
    thread.start()

    active_stresses['cpu'] = {
      target: target,
      duration: duration,
      started_at: time.time(),
      thread: thread
    }

    RETURN {status: "started", target_cpu: target}

ENDPOINT: GET /compute/pi
  PARAMETERS: iterations (default: 10000000)
  PURPOSE: CPU-intensive computation for distributed load testing

  LOGIC:
    # Monte Carlo Pi calculation
    inside_circle = 0
    FOR i FROM 0 TO iterations:
      x = random()
      y = random()
      IF x² + y² < 1:
        inside_circle += 1

    pi_estimate = 4.0 * inside_circle / iterations
    RETURN {pi: pi_estimate, iterations: iterations}

  USE CASE:
    Alpine Pi nodes send GET /compute/pi requests
    Docker Swarm load balancer distributes across replicas
    Each replica performs calculation → CPU load distributed evenly
```

CPU STRESS IMPLEMENTATION (stress/cpu_stress.py):

```
FUNCTION cpu_stress_worker(target_percent, duration_seconds, ramp_seconds):
  # Calculate number of processes needed
  num_cpus = psutil.cpu_count()
  target_load = target_percent / 100.0
  num_processes = max(1, int(num_cpus * target_load))

  LOG "Starting CPU stress: target={target_percent}%, processes={num_processes}"

  # Gradual ramp-up (spawn processes one at a time)
  processes = []
  ramp_steps = num_processes
  delay_per_process = ramp_seconds / ramp_steps if ramp_steps > 0 else 0

  FOR i FROM 0 TO num_processes:
    # Spawn busy-loop process
    process = multiprocessing.Process(target=busy_loop, args=(duration_seconds,))
    process.start()
    processes.append(process)

    LOG "Spawned process {i+1}/{num_processes}"
    SLEEP delay_per_process

  LOG "All CPU processes spawned, maintaining load for {duration_seconds}s"

  # Wait for duration
  SLEEP duration_seconds

  # Terminate all processes
  FOR process IN processes:
    process.terminate()
    process.join(timeout=5)
    IF process.is_alive():
      process.kill()

  LOG "CPU stress completed"

FUNCTION busy_loop(duration):
  # Pure CPU computation (no I/O)
  start = time.time()
  WHILE (time.time() - start) < duration:
    # Compute prime numbers (CPU-intensive)
    FOR n FROM 2 TO 1000:
      is_prime = True
      FOR i FROM 2 TO sqrt(n):
        IF n % i == 0:
          is_prime = False
          BREAK
```

Why This Gradual Approach?
- Spawning all processes at once creates sudden spike in Grafana
- Gradual spawn shows smooth ramp-up (better for visualization)
- Mimics realistic application behavior under increasing load
- Allows observation of monitoring agent detecting threshold violations

================================================================================
3. TESTING METHODOLOGY
================================================================================

3.1 Test Environment Setup

PHYSICAL INFRASTRUCTURE:
- Docker Swarm cluster: 1 master + 4 workers (x86_64 Ubuntu)
- Monitoring node: Raspberry Pi with InfluxDB + Grafana (ARM)
- Load testing: 4 Raspberry Pi 1.2B+ with Alpine Linux (ARM)
- Control machine: macOS for SSH orchestration

DEPLOYMENT SEQUENCE:
1. Create Docker overlay network (swarmguard-net)
2. Deploy monitoring agents (5 services, one per node)
3. Deploy recovery manager (1 service on master)
4. Deploy web-stress test application (1 initial replica)
5. Verify all services healthy
6. Access Grafana dashboards for visualization

NETWORK TOPOLOGY:
All nodes on 192.168.2.0/24 network
- Master: 192.168.2.50
- Workers: 192.168.2.51-54
- Monitoring: 192.168.2.61
- Alpine load: alpine-1 through alpine-4 (SSH aliases)

3.2 Test Scenarios and Procedures

TEST CASE 1: Scenario 1 Validation (Migration)
Objective: Verify zero-downtime container migration

Procedure:
1. Deploy web-stress with 1 replica
2. Verify initial placement (e.g., worker-3)
3. Trigger CPU stress (no network stress):
   curl "http://IP:8080/stress/cpu?target=85&duration=120&ramp=30"
4. Monitor recovery manager logs for migration detection
5. Verify new container deployed on different node (e.g., worker-4)
6. Verify old container removed
7. Start Alpine Pi continuous traffic BEFORE stress (Attempt 25):
   ./alpine_scenario1_visualize.sh 85 1200 60
8. Monitor for failed requests (indicates downtime)
9. Calculate MTTR from logs (T0 to T5)

Expected Results:
- Migration detected within 10-15 seconds (2× poll interval + breach count)
- New container on different node within 6-10 seconds
- Zero failed requests from Alpine nodes (100% uptime)
- MTTR < 10 seconds
- Grafana shows container moving from worker-3 to worker-4
- Both containers briefly visible during transition (zero-downtime proof)

TEST CASE 2: Scenario 2 Validation (Horizontal Scaling)
Objective: Verify automatic scale-up and scale-down

Procedure Phase 1 (Scale-Up):
1. Deploy web-stress with 1 replica
2. Trigger combined stress with high network (Attempt 28):
   ./alpine_scenario2_visualize.sh 85 1000 75 60 15
   # 85% CPU, 1000MB memory, 75Mbps network, 60s ramp, 15 users/node
3. Alpine nodes send continuous CPU-intensive /compute/pi requests
4. Monitor for scale-up events (1→2→3 replicas)
5. Observe load distribution in Grafana
6. Verify cooldown prevents excessive scaling (60s between scale-ups)

Procedure Phase 2 (Scale-Down):
1. Let stress complete (or manually stop)
2. Wait for idle detection (60s background check interval)
3. Wait for sustained idle period (180s)
4. Verify scale-down occurs (3→2→1 replicas)
5. Verify scale-down cooldown (180s between scale-downs)

Expected Results:
- Scale-up triggered within 10-15 seconds of threshold breach
- Each scale-up completes in < 1 second (0.01s achieved)
- Load distributed evenly across replicas (Grafana shows equal CPU/MEM/NET)
- Alpine traffic load-balanced by Docker Swarm
- Scale-down after 240s total (60s check + 180s idle)
- Each scale-down completes in < 1 second (0.02s achieved)
- Minimum 1 replica maintained

TEST CASE 3: MTTR Comparison (Proactive vs Reactive)
Objective: Demonstrate improvement over Docker Swarm's reactive recovery

Baseline Test (Reactive):
1. Disable recovery manager (docker service scale recovery-manager=0)
2. Deploy web-stress with 1 replica
3. Identify container ID on node
4. Kill container process: docker exec {container_id} kill -9 1
5. Measure time from kill to new container healthy
6. Record downtime (service unavailable period)

Proactive Test:
1. Enable recovery manager
2. Trigger Scenario 1 (CPU stress, low network)
3. Measure time from threshold breach to migration complete
4. Record downtime (should be zero)

Comparison Metrics:
- Baseline MTTR: 10-15 seconds (measured)
- Proactive MTTR: 6.08 seconds (achieved)
- Improvement: ~50% reduction
- Baseline Downtime: 10-15 seconds
- Proactive Downtime: 0 seconds
- Improvement: 100% reduction (zero downtime achieved)

3.3 Measurement and Instrumentation

TIMING MEASUREMENTS:
All timestamps logged in recovery manager for precise MTTR calculation:

T0: Threshold breach detected by monitoring agent
T1: Alert sent to recovery manager (network transmission)
T2: Alert received by recovery manager (Flask endpoint)
T3: Decision made (rule evaluation, cooldown check)
T4: Action initiated (Docker API call)
T5: New container/replica ready (health checks passed)
T6: Old container removed (Scenario 1 only)

Latency Calculations:
- Alert Latency: T2 - T1 (target: < 1s, achieved: 7-9ms)
- Decision Latency: T3 - T2 (target: < 1s, achieved: < 100ms)
- Action Execution: T5 - T4 (target: 3-8s, achieved: 6-8s)
- Total MTTR: T5 - T0 (target: < 10s, achieved: 6.08s for migration)

GRAFANA DASHBOARDS:
Real-time visualization of:
- Per-container CPU/Memory/Network metrics (line graphs)
- Per-node resource utilization (aggregated)
- Recovery action timeline (annotations)
- Alert firing events (vertical lines)
- Replica count over time (step graph)

ZERO-DOWNTIME VALIDATION:
Alpine Pi continuous health checks:
  WHILE test_running:
    response = curl http://service:8080/health
    timestamp = now()
    IF response.status_code == 200:
      LOG "{timestamp} 200 OK"
      success_count += 1
    ELSE:
      LOG "{timestamp} FAILED ({response.status_code})"
      failure_count += 1
    SLEEP 0.5

  uptime_percent = (success_count / total_requests) * 100
  downtime_seconds = failure_count * 0.5

Expected: 100% uptime, 0 downtime

================================================================================
END OF PART 3
================================================================================
